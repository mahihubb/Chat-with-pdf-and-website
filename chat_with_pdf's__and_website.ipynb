{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahihubb/Chat-with-pdf-and-website/blob/master/chat_with_pdf's__and_website.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZG4ro4r49zlr"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFQKtzAotSt9",
        "outputId": "7d2226c6-e10c-4f3f-abcc-b03e6c2562ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask Your PDF\n",
            "Enter the paths of your files separated by commas: /content/drive/MyDrive/Chat with PDF and Website/Sample2.pdf\n",
            "Process files? (yes/no): yes\n",
            "Ask a question about your files (or type 'exit' to quit): what is task1?\n",
            "\n",
            "Chat History:\n",
            "User: what is task1?\n",
            "Bot: Implement a Retrieval-Augmented Generation (RAG) pipeline that allows users to interact with semi-structured data in multiple PDF files\n",
            "Ask a question about your files (or type 'exit' to quit): what is task 2?\n",
            "\n",
            "Chat History:\n",
            "User: what is task1?\n",
            "Bot: Implement a Retrieval-Augmented Generation (RAG) pipeline that allows users to interact with semi-structured data in multiple PDF files\n",
            "User: what is task 2?\n",
            "Bot: Chat with Website Using RAG Pipeline\n",
            "Ask a question about your files (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS  # Facebook AI similarity search\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "import docx\n",
        "import os\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.callbacks import StdOutCallbackHandler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "def main():\n",
        "    load_dotenv()\n",
        "    print(\"Ask Your PDF\")\n",
        "\n",
        "    conversation = None\n",
        "    chat_history = None\n",
        "    process_complete = False\n",
        "\n",
        "    # File upload simulation\n",
        "    uploaded_files = input(\"Enter the paths of your files separated by commas: \").split(',')\n",
        "    process = input(\"Process files? (yes/no): \").strip().lower() == 'yes' #To end or continue with the process...\n",
        "\n",
        "    if process:\n",
        "        files_text = get_files_text(uploaded_files)\n",
        "        text_chunks = get_text_chunks(files_text)\n",
        "        vectorstore = get_vectorstore(text_chunks)\n",
        "        conversation = get_conversation_chain(vectorstore)\n",
        "        process_complete = True\n",
        "\n",
        "    if process_complete:\n",
        "        while True:\n",
        "            user_question = input(\"Ask a question about your files (or type 'exit' to quit): \").strip()\n",
        "            if user_question.lower() == 'exit':\n",
        "                break\n",
        "            handle_user_input(user_question, conversation)\n",
        "\n",
        "def get_files_text(uploaded_files):\n",
        "    text = \"\"\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_extension = os.path.splitext(uploaded_file)[1]\n",
        "        if file_extension == \".pdf\":\n",
        "            text += get_pdf_text(uploaded_file)\n",
        "        elif file_extension == \".docx\":\n",
        "            text += get_docx_text(uploaded_file)\n",
        "        else:\n",
        "            text += get_csv_text(uploaded_file)\n",
        "    return text\n",
        "\n",
        "def get_pdf_text(pdf_path):\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_docx_text(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    all_text = [docpara.text for docpara in doc.paragraphs]\n",
        "    return ' '.join(all_text)\n",
        "\n",
        "def get_csv_text(file_path):\n",
        "    # Placeholder for CSV text extraction\n",
        "    return \"CSV processing is not implemented yet.\"\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=900,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    knowledge_base = FAISS.from_texts(text_chunks, embeddings)\n",
        "    return knowledge_base\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "    llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0.7, \"max_length\": 64})\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        verbose=False  # Suppress debug output\n",
        "    )\n",
        "\n",
        "\n",
        "def handle_user_input(user_question, conversation):\n",
        "    response = conversation({'question': user_question})\n",
        "    chat_history = response['chat_history']\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(f\"User: {message.content}\")\n",
        "        else:\n",
        "            print(f\"Bot: {message.content}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKraLFNxWd5b",
        "outputId": "536beedd-c26d-4c61-de1e-c1fc8ce81915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with Websites\n",
            "\n",
            "Loading model: t5-small from Hugging Face...\n",
            "Loading the website content...\n",
            "Content loaded successfully.\n",
            "\n",
            "ai: Hello, I am a bot. How can I help you?\n",
            "ai: The title of the website is Google\n",
            "ai: Google 2024 - Privacy - Terms GoogleSearch Images Maps Play YouTube News G\n",
            "ai: Question: exit\n",
            "human: exit\n",
            "ai: Exiting. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import requests\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def get_page_title(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.title.string if soup.title else \"No title found\"\n",
        "    return title\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def get_vectorstore_from_url(url):\n",
        "    loader = WebBaseLoader(url)\n",
        "    document = loader.load()\n",
        "    soup = BeautifulSoup(document[0].page_content, 'html.parser')\n",
        "    title = soup.title.string if soup.title else \"Title not found\"\n",
        "\n",
        "    document_chunks = [Document(page_content=title)]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    document_chunks.extend(text_splitter.split_documents(document))\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "    vector_store = Chroma.from_documents(document_chunks, embeddings)\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "def load_model_from_huggingface(model_name=\"t5-small\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text_with_model(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def get_response(user_input, vector_store, model, tokenizer):\n",
        "    retriever = vector_store.as_retriever()\n",
        "    docs = retriever.get_relevant_documents(user_input)\n",
        "\n",
        "    if not docs:\n",
        "        return \"I couldn't find any relevant information.\"\n",
        "\n",
        "    combined_docs = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    response = generate_text_with_model(combined_docs + \"\\nQuestion: \" + user_input, model, tokenizer)\n",
        "\n",
        "    return response\n",
        "\n",
        "def main():\n",
        "    print(\"Chat with Websites\\n\")\n",
        "\n",
        "    model_name = \"t5-small\"\n",
        "    print(f\"Loading model: {model_name} from Hugging Face...\")\n",
        "    local_llm_model, local_llm_tokenizer = load_model_from_huggingface(model_name)\n",
        "\n",
        "    website_url = \"https://www.google.com\"\n",
        "    if not website_url:\n",
        "        print(\"Please provide a valid URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"Loading the website content...\")\n",
        "    vector_store = get_vectorstore_from_url(website_url)\n",
        "    print(\"Content loaded successfully.\\n\")\n",
        "\n",
        "    chat_history = [AIMessage(content=\"Hello, I am a bot. How can I help you?\")]\n",
        "    print(f\"ai: {chat_history[-1].content}\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"human: \")\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ai: Exiting. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if 'title' in user_query.lower():\n",
        "            title = get_page_title(website_url)\n",
        "            print(f\"ai: The title of the website is {title}\")\n",
        "            chat_history.append(HumanMessage(content=user_query))\n",
        "            chat_history.append(AIMessage(content=f\"The title of the website is {title}\"))\n",
        "        else:\n",
        "            try:\n",
        "                response = get_response(user_query, vector_store, local_llm_model, local_llm_tokenizer)\n",
        "                chat_history.append(HumanMessage(content=user_query))\n",
        "                chat_history.append(AIMessage(content=response))\n",
        "                print(f\"ai: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ai: Error during response generation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBjIHCkctGgf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1BnBYkp9gdVZWD7l2umc_ObXy0TBRJrUs",
      "authorship_tag": "ABX9TyNKECfEOqrUYCX48mjHbCmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}